{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shared_notebook_utils import *\n",
    "from statsmodels.sandbox.stats.runs import mcnemar\n",
    "seaborn.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "datasets = load_datasets(dirnames=['freesound_loops_db_200'], clean=True, exclude_files=['analysis_freesound_extractor_04.json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Edit output of Percival14_bpm according to confidence\n",
    "def compute_confidence_measure(estimated_bpm,\n",
    "                       duration_samples,\n",
    "                       start_effective_duration,\n",
    "                       end_effective_duration,\n",
    "                       sample_rate=44100, beat_range=range(1, 128), k=0.5):\n",
    "    if estimated_bpm == 0:\n",
    "        # This condition is to skip computing other steps if estimated bpm is 0, we already know that the\n",
    "        # output will be 0\n",
    "        return 0\n",
    "\n",
    "    durations_to_check = [\n",
    "        duration_samples,\n",
    "        duration_samples - start_effective_duration,\n",
    "        end_effective_duration,\n",
    "        end_effective_duration - start_effective_duration\n",
    "    ]\n",
    "\n",
    "    beat_duration = (60.0 * sample_rate)/estimated_bpm\n",
    "    L = [beat_duration * n for n in beat_range]\n",
    "    thr_lambda = k * beat_duration\n",
    "    confidences = list()\n",
    "    for duration in durations_to_check:\n",
    "        delta_l = min([abs(l - duration) for l in L])\n",
    "        if delta_l > thr_lambda:\n",
    "            confidences.append(0.0)\n",
    "        else:\n",
    "            confidences.append(1.0 - float(delta_l) / thr_lambda)\n",
    "    return max(confidences)\n",
    "\n",
    "\n",
    "def confidence_helper(estimated_bpm, sound):\n",
    "    return compute_confidence_measure(\n",
    "        estimated_bpm,  \n",
    "        item['analysis']['basic_audio_properties']['length_samples'],\n",
    "        item['analysis']['basic_audio_properties']['start_effective_duration'], \n",
    "        item['analysis']['basic_audio_properties']['end_effective_duration']\n",
    "    )\n",
    "\n",
    "\n",
    "# Iterate over all instances in all datasets and for all methods\n",
    "for dataset in datasets:\n",
    "    methods_to_compare = ['Percival14_bpm']\n",
    "    for key, item in dataset.data.items():\n",
    "        for method in methods_to_compare:\n",
    "            estimated_bpm = int(round(item['analysis'][method]['bpm_raw']))\n",
    "            item['analysis'][method]['confidence_ffont'] = confidence_helper(estimated_bpm, item)\n",
    "            confidences = list()\n",
    "            WINDOW = 1\n",
    "            window_range = range(-WINDOW, WINDOW + 1)\n",
    "            for candidate in [estimated_bpm + x for x in range(-WINDOW, WINDOW + 1)]:\n",
    "                confidence = confidence_helper(candidate, item)\n",
    "                confidences.append(confidence)\n",
    "            \n",
    "            i = confidences.index(max(confidences))\n",
    "            final_bpm = estimated_bpm + window_range[i]\n",
    "            item['analysis'][method + '2'] = dict()\n",
    "            item['analysis'][method + '2']['bpm_raw'] = final_bpm\n",
    "            item['analysis'][method + '2']['confidence_ffont'] = confidence_helper(estimated_bpm, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "General tempo estimation results (freesound_loops_db_200)\n",
      "**********************************************************\n",
      "\n",
      "\n",
      "Method            Accuracy 1e   Accuracy 1   Accuracy 2   \n",
      "----------------------------------------------------------\n",
      "Percival14_bpm    53.54         \u001b[36m64.14        \u001b[0m\u001b[36m84.85        \u001b[0m\n",
      "Percival14_bpm2   \u001b[36m54.55         \u001b[0m\u001b[36m64.14        \u001b[0m\u001b[36m84.85        \u001b[0m\n",
      "\n",
      "\n",
      "General tempo estimation results (freesound_loops_db_200) - confidence > 1.00\n",
      "******************************************************************************\n",
      "\n",
      "\n",
      "Method            Accuracy 1e   Accuracy 1   Accuracy 2   \n",
      "----------------------------------------------------------\n",
      "Percival14_bpm    \u001b[36m62.07         \u001b[0m\u001b[36m62.07        \u001b[0m\u001b[36m98.28        \u001b[0m\n",
      "Percival14_bpm2   \u001b[36m62.07         \u001b[0m\u001b[36m62.07        \u001b[0m\u001b[36m98.28        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for count, dataset in enumerate(datasets):\n",
    "    print title(\"\\nGeneral tempo estimation results (%s)\" % dataset.short_name, c='*')\n",
    "    methods_to_compare = ['Percival14_bpm', 'Percival14_bpm2']\n",
    "    table_header = ['Method', 'Accuracy 1e', 'Accuracy 1', 'Accuracy 2']\n",
    "    table_rows = list()\n",
    "\n",
    "    for method in methods_to_compare:\n",
    "        try:\n",
    "            table_row = [method]\n",
    "            for accuracy_func in accuracy1e, accuracy1, accuracy2:\n",
    "                method_results = accuracy_func(dataset.data, method)\n",
    "                table_row.append(100 * basic_statistics(method_results)['avg'])\n",
    "            table_rows.append(table_row)\n",
    "        except IndexError:\n",
    "            print \"Warning: Skipping method %s (analsyis not found in dataset)\" % method\n",
    "            continue\n",
    "    print \"\"\n",
    "    print_table(table_header, table_rows, sort_column=3, highlight_max=True)\n",
    "    \n",
    "# Now for confidence above threhsold\n",
    "thr = 1.0\n",
    "for count, dataset in enumerate(datasets):\n",
    "    print title(\"\\nGeneral tempo estimation results (%s) - confidence > %.2f\" % (dataset.short_name, thr), c='*')\n",
    "    methods_to_compare = ['Percival14_bpm', 'Percival14_bpm2']\n",
    "    table_header = ['Method', 'Accuracy 1e', 'Accuracy 1', 'Accuracy 2']\n",
    "    table_rows = list()\n",
    "\n",
    "    for method in methods_to_compare:\n",
    "        try:\n",
    "            table_row = [method]\n",
    "            for accuracy_func in accuracy1e, accuracy1, accuracy2:\n",
    "                filtered_data = dataset.filter_data([('analysis.%s.%s__>=' % (method, 'confidence_ffont'), thr)]).data\n",
    "                method_results = accuracy_func(filtered_data, method)\n",
    "                table_row.append(100 * basic_statistics(method_results)['avg'])\n",
    "            table_rows.append(table_row)\n",
    "        except IndexError:\n",
    "            print \"Warning: Skipping method %s (analsyis not found in dataset)\" % method\n",
    "            continue\n",
    "    print \"\"\n",
    "    print_table(table_header, table_rows, sort_column=3, highlight_max=True)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
