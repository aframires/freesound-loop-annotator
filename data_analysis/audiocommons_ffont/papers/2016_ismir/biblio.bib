@inproceedings{Sebastian2015,
author = {B{\"{o}}ck, Sebastian and Krebs, Florian and Widmer, Gerhard},
booktitle = {Proc. of the Int. Conf. on Music Information Retrieval (ISMIR)},
file = {:Users/ffont/Documents/Mendeley Desktop/B{\"{o}}ck, Krebs, Widmer - 2015 - Accurate Tempo Estimation based on Recurrent Neural Networks and Resonating Comb Filters.pdf:pdf},
title = {{Accurate Tempo Estimation based on Recurrent Neural Networks and Resonating Comb Filters}},
year = {2015}
}
@inproceedings{Bogdanov2013,
abstract = {We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.},
author = {Bogdanov, Dmitry and Wack, Nicolas and G{\'{o}}mez, Emilia and Gulati, Sankalp and Herrera, Perfecto and Mayor, Oscar and Roma, Gerard and Salamon, Justin and Zapata, Jose and Serra, Xavier},
booktitle = {Proc. of the Int. Conf. on Music Information Retrieval (ISMIR)},
file = {:Users/ffont/Documents/Mendeley Desktop/Bogdanov et al. - 2013 - ESSENTIA An audio analysis library for music information retrieval.pdf:pdf},
pages = {493--498},
title = {{ESSENTIA: An audio analysis library for music information retrieval}},
year = {2013}
}
@article{Davies2007a,
author = {Davies, Matthew EP and Plumbley, Mark D},
file = {:Users/ffont/Documents/Mendeley Desktop/Davies, Plumbley - 2007 - Context-dependent beat tracking of musical audio.pdf:pdf},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
number = {3},
pages = {1009--1020},
title = {{Context-dependent beat tracking of musical audio}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4100674},
volume = {15},
year = {2007}
}
@article{Degara2012,
abstract = {A new probabilistic framework for beat tracking of musical audio is presented. The method estimates the time between consecutive beat events and exploits both beat and non-beat information by explicitly modeling non-beat states. In addition to the beat times, a measure of the expected accuracy of the estimated beats is provided. The quality of the observations used for beat tracking is measured and the reliability of the beats is automatically calculated. A k -nearest neighbor regression algorithm is proposed to predict the accuracy of the beat estimates. The performance of the beat tracking system is statistically evaluated using a database of 222 musical signals of various genres. We show that modeling non-beat states leads to a significant increase in performance. In addition, a large experiment where the parameters of the model are automatically learned has been completed. Results show that simple approximations for the parameters of the model can be used. Furthermore, the performance of the system is compared with existing algorithms. Finally, a new perspective for beat tracking evaluation is presented. We show how reliability information can be successfully used to increase the mean performance of the proposed algorithm and discuss how far automatic beat tracking is from human tapping.},
author = {Degara, Norberto and Rua, Enrique Argones and Pena, Antonio and Torres-Guijarro, Soledad and Davies, Matthew EP and Plumbley, Mark D},
doi = {10.1109/TASL.2011.2160854},
file = {:Users/ffont/Documents/Mendeley Desktop/Degara et al. - 2012 - Reliability-Informed Beat Tracking of Musical Signals.pdf:pdf},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {{\$}k{\$}-nearest neighbor ({\$}k{\$}-NN) regression,Accuracy,Beat-tracking,Estimation,Hidden Markov models,K-nearest neighbor regression algorithm,Materials,Prediction algorithms,Probabilistic logic,Reliability,acoustic signal processing,audio signal processing,beat quality,beat tracking,beat-tracking reliability,human tapping,music,music signal processing,musical audio signal,non-beat states modeling,object tracking,probability,regression analysis,reliability,statistical analysis},
number = {1},
pages = {290--301},
title = {{Reliability-Informed Beat Tracking of Musical Signals}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5934584},
volume = {20},
year = {2012}
}
@article{Dixon2001,
abstract = {We describe a computer program which is able to estimate the tempo and the times of musical beats in expressively performed music. The input data may be either digital audio or a symbolic representation of music such as MIDI. The data is processed off-line to detect the salient rhythmic events and the timing of these events is analysed to generate hypotheses of the tempo at various metrical levels. Based on these tempo hypotheses, a multiple hypothesis search finds the sequence of beat times which has the best fit to the rhythmic events. We show that estimating the perceptual salience of rhythmic events significantly improves the results. No prior knowledge of the tempo, meter or musical style is assumed; all required information is derived from the data. Results are presented for a range of different musical styles, including classical, jazz, and popular works with a variety of tempi and meters. The system calculates the tempo correctly in most cases, the most common error being a doubling or halving of the tempo. The calculation of beat times is also robust. When errors are made concerning the phase of the beat, the system recovers quickly to resume correct beat tracking, despite the fact that there is no high level musical knowledge encoded in the system.},
author = {Dixon, Simon},
doi = {10.1076/jnmr.30.1.39.7119},
file = {:Users/ffont/Documents/Mendeley Desktop/Dixon - 2001 - Automatic extraction of tempo and beat from expressive performances.pdf:pdf},
issn = {09298215},
journal = {Journal of New Music Research},
pages = {39--58},
title = {{Automatic extraction of tempo and beat from expressive performances}},
url = {http://www.informaworld.com/openurl?genre=article{\&}doi=10.1076/jnmr.30.1.39.7119{\&}magic=crossref},
volume = {30},
year = {2001}
}
@inproceedings{Elowsson2013,
author = {Elowsson, Anders and Friberg, Anders and Madison, Guy and Paulin, Johan},
booktitle = {Proc. of the Int. Conf. on Music Information Retrieval (ISMIR)},
file = {:Users/ffont/Documents/Mendeley Desktop/Elowsson et al. - 2013 - Modelling the Speed of Music using Features from HarmonicPercussive Separated Audio.pdf:pdf},
pages = {481--486},
title = {{Modelling the Speed of Music using Features from Harmonic/Percussive Separated Audio}},
year = {2013}
}
@inproceedings{Font2013,
abstract = {Freesound is an online collaborative sound database where people with diverse interests share recorded sound samples under Creative Commons licenses. It was started in 2005 and it is being maintained to support diverse research projects and as a service to the overall research and artistic community. In this demo we want to introduce Freesound to the multimedia community and show its potential as a research resource. We begin by describing some general aspects of Freesound, its architecture and functionalities, and then explain potential usages that this framework has for research applications.},
author = {Font, Frederic and Roma, Gerard and Serra, Xavier},
booktitle = {Proc. of the ACM Int. Conf. on Multimedia (ACM MM)},
doi = {10.1145/2502081.2502245},
file = {:Users/ffont/Documents/Mendeley Desktop/Font, Roma, Serra - 2013 - Freesound technical demo.pdf:pdf},
isbn = {9781450324045},
keywords = {audio clips,freesound,online databases,sound},
pages = {411--412},
title = {{Freesound technical demo}},
url = {http://dl.acm.org/citation.cfm?doid=2502081.2502245},
year = {2013}
}
@article{Gainza2011,
abstract = {In this paper, a novel tempo detection system is presented, which suggests the use of a hybrid multiband decomposition. The model tracks the periodicities of different signal property changes that manifest within different frequency bands by using the most appropriate onset/transient detectors for each frequency band. In addition, the proposed system applies a novel method to weight tempo candidates. Each contribution is evaluated by comparing the presented system against existing approaches using three different databases that comprises 1638 songs. These databases include the two publicly available database of songs used in the tempo evaluation contest of ISMIR 2004. These songs are used in order to compare the proposed approach against four recent existing approaches and also against the participants of the tempo detection contest of ISMIR 2004. The results show that the presented approach provides an improvement over existing techniques.},
author = {Gainza, Mikel and Coyle, Eugene},
doi = {10.1109/TASL.2010.2045182},
file = {:Users/ffont/Documents/Mendeley Desktop/Gainza, Coyle - 2011 - Tempo detection using a hybrid multiband approach.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Onset detection,periodicity detection,rhythm description,tempo detection},
number = {1},
pages = {57--68},
title = {{Tempo detection using a hybrid multiband approach}},
volume = {19},
year = {2011}
}
@inproceedings{Gkiokas2012,
author = {Gkiokas, Aggelos and Katsouros, Vassilis and Carayannis, George and Stafylakis, Themos},
booktitle = {Proc. of the Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/ffont/Documents/Mendeley Desktop/Gkiokas et al. - 2012 - Music Tempo Estimation and Beat Tracking By Applying Source Separation and Metrical Relations.pdf:pdf},
isbn = {9781467300469},
pages = {421--424},
title = {{Music Tempo Estimation and Beat Tracking By Applying Source Separation and Metrical Relations}},
volume = {7},
year = {2012}
}
@article{Gouyon2006,
author = {Gouyon, Fabien and Klapuri, Anssi and Dixon, Simon and Alonso, Miguel and Tzanetakis, George and Uhle, Christian and Cano, Pedro},
file = {:Users/ffont/Documents/Mendeley Desktop/Gouyon et al. - 2006 - An Experimental Comparison of Audio Tempo Induction Algorithms.pdf:pdf},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
number = {5},
pages = {1832--1844},
title = {{An Experimental Comparison of Audio Tempo Induction Algorithms}},
volume = {14},
year = {2006}
}
@article{Grosche2010,
abstract = {The extraction of tempo and beat Inform. from music recordings constitutes a challenging task in particular for non-percussive music with soft note onsets and time-varying tempo. In this paper, we introduce a novel mid-level representation that captures musically meaningful local pulse Inform. even for the case of complex music. Our main idea is to derive for each time position a sinusoidal kernel that best explains the local periodic nature of a previously extracted note onset representation. Then we employ an overlap-add technique accumulating all these kernels over time to obtain a single function that reveals the predominant local pulse (PLP). Our concept introduces a high degree of robustness to noise and distortions resulting from weak and blurry onsets. Furthermore, the resulting PLP curve reveals the local pulse Inform. even in the presence of continuous tempo changes and indicates a kind of confidence in the periodicity estimation. As further contribution, we show how our PLP concept can be used as a flexible tool for enhancing tempo estimation and beat tracking. The practical relevance of our approach is demonstrated by extensive experiments based on music recordings of various genres.},
author = {Grosche, Peter and M{\"{u}}ller, Meinard},
doi = {10.1109/TASL.2010.2096216},
file = {:Users/ffont/Documents/Mendeley Desktop/Grosche, M{\"{u}}ller - 2011 - Extracting Predominant Local Pulse Information from Music Recordings.pdf:pdf},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
number = {6},
pages = {1688--1701},
title = {{Extracting Predominant Local Pulse Information from Music Recordings}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5654580},
volume = {19},
year = {2011}
}
@article{Klapuri2006,
author = {Klapuri, Anssi and Eronen, Antti and Astola, Jaakko},
file = {:Users/ffont/Documents/Mendeley Desktop/Klapuri, Eronen, Astola - 2006 - Analysis of the meter of musical signals.pdf:pdf},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
number = {1},
pages = {342--355},
title = {{Analysis of the meter of musical signals}},
volume = {14},
year = {2006}
}
@book{Muller2015,
author = {M{\"{u}}ller, Meinard},
doi = {10.1007/978-3-319-21945-5},
file = {:Users/ffont/Documents/Mendeley Desktop/M{\"{u}}ller - 2015 - Fundamentals of Music Processing.pdf:pdf},
isbn = {978-3-319-21944-8},
publisher = {Springer},
title = {{Fundamentals of Music Processing}},
url = {http://link.springer.com/10.1007/978-3-319-21945-5},
year = {2015}
}
@inproceedings{Oliveira2010,
abstract = {This paper describes a tempo induction and beat track- ing system based on the efficient strategy (initially intro- duced in the BeatRoot system [Dixon S., “Automatic ex- traction of tempo and beat from expressive performances.” Journal of New Music Research, 30(1):39-58, 2001]) of competing agents processing musical input sequentially and considering parallel hypotheses regarding tempo and beats. In this paper, we propose to extend this strategy to the causal processing of continuous input data. The main rea- sons for this are threefold: providing more robustness to potentially noisy input data, permitting the parallel consid- eration of a number of low-level frame-based features as input, and opening the way to real-time uses of the system (as e.g. for a mobile robotic platform). The system is implemented in C++, permitting faster than real-time processing of audio data. It is integrated in the MARSYAS framework, and is therefore available under GPL for users and/or researchers. Detailed evaluation of the causal and non-causal ver- sions of the system on common benchmark datasets show performances reaching those of state-of-the-art beat track- ers. We propose a series of lines for future work based on careful analysis of the results.},
author = {Oliveira, Jo{\~{a}}o L and Gouyon, Fabien and Martins, Luis Gustavo and Reis, Luis Paolo},
booktitle = {Proc. of the Int. Conf. on Music Information Retrieval (ISMIR)},
file = {:Users/ffont/Documents/Mendeley Desktop/Oliveira et al. - 2010 - IBT A real-time tempo and beat tracking system.pdf:pdf},
isbn = {978-90-393-5381-3},
pages = {291--296},
title = {{IBT: A real-time tempo and beat tracking system}},
url = {http://repositorio.ucp.pt/handle/10400.14/4800},
year = {2010}
}
@article{Percival2014,
author = {Percival, Graham and Tzanetakis, George},
file = {:Users/ffont/Documents/Mendeley Desktop/Percival, Tzanetakis - 2014 - Streamlined tempo estimation based on autocorrelation and cross-correlation with pulses.pdf:pdf},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
number = {12},
pages = {1765--1776},
title = {{Streamlined tempo estimation based on autocorrelation and cross-correlation with pulses}},
url = {http://dl.acm.org/citation.cfm?id=2719957},
volume = {22},
year = {2014}
}
@inproceedings{Quinton2016,
author = {Quinton, Elio and Sandler, Mark and Dixon, Simon},
booktitle = {Proc. of the Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/ffont/Documents/Mendeley Desktop/Quinton, Sandler, Dixon - 2016 - Estimation of the Reliability of Multiple Rhythm Features Extraction from a Single Descriptor.pdf:pdf},
isbn = {9781479999880},
pages = {256--260},
title = {{Estimation of the Reliability of Multiple Rhythm Features Extraction from a Single Descriptor}},
year = {2016}
}
@phdthesis{Roma2015,
author = {Roma, Gerard},
file = {:Users/ffont/Documents/Mendeley Desktop/Roma - 2015 - Algorithms and representations for supporting online music creation with large-scale audio databases.pdf:pdf},
school = {Universitat Pompeu Fabra},
title = {{Algorithms and representations for supporting online music creation with large-scale audio databases}},
year = {2015}
}
@inproceedings{Tzanetakis2002,
abstract = {Musical signals exhibit periodic temporal structure that create the sensation of rhythm. In order to model, analyze, and retrieve musical signals it is important to automatically extract rhythmic information. To somewhat simplify the problem, automatic algorithms typically only extract information about the main beat of the signal which can be loosely defined as the regular periodic sequence of pulses corresponding to where a human would tap his foot while listening to the music. In these algorithms, the beat is characterized by its frequency (tempo), phase (accent locations) and a confidence measure about its detection.},
author = {Tzanetakis, George and Essl, Georg and Cook, Perry},
booktitle = {Proc. of the Int. Conf. on Digital Audio Effects (DAFx)},
doi = {10.1.1.20.6271},
file = {:Users/ffont/Documents/Mendeley Desktop/Tzanetakis, Essl, Cook - 2002 - Human Perception and Computer Extraction of Musical Beat Strength.pdf:pdf},
pages = {257--261},
title = {{Human Perception and Computer Extraction of Musical Beat Strength}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.6271},
year = {2002}
}
@inproceedings{Wu2014,
author = {Wu, Fu-Hai Frank and Jang, Jyh-Shing Roger},
booktitle = {Proc. of the Mediterranean Conf. on Control and Automation (MED)},
file = {:Users/ffont/Documents/Mendeley Desktop/Wu, Jang - 2014 - A Supervised Learning Method for Tempo Estimation of Musical Audio.pdf:pdf},
isbn = {9781479958993},
pages = {599--604},
title = {{A Supervised Learning Method for Tempo Estimation of Musical Audio}},
year = {2014}
}
@article{Zapata2014,
author = {Zapata, Jose R and Davies, Matthew EP and G{\'{o}}mez, Emilia},
doi = {10.1109/TASLP.2014.2305252},
file = {:Users/ffont/Documents/Mendeley Desktop/Zapata, Davies, G{\'{o}}mez - 2014 - Multi-Feature Beat Tracking.pdf:pdf},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
number = {4},
pages = {816--825},
title = {{Multi-Feature Beat Tracking}},
volume = {22},
year = {2014}
}
@inproceedings{Zapata2011,
abstract = {The automatic analysis of musical tempo from audio is still an open research task in the Music Information Retrieval(MIR) community. The goal of this paper is to provide an updated comparative evaluation of different methods for audio tempo estimation. We overview, following the same block diagram, 23 documented methods. We then analyze their accuracy, error distribution and statistical differencces, and we discuss which strategies can provide better performance for different input material. We then take advantage of their complementarity to imporove the results by combining different methods, and we finally analyze the limitations of current approaches and give some ideas for future work on the task.},
author = {Zapata, Jose R and G{\'{o}}mez, Emilia},
booktitle = {Proc. of the AES Int. Conf. on Semantic Audio},
file = {:Users/ffont/Documents/Mendeley Desktop/Zapata, G{\'{o}}mez - 2011 - Comparative Evaluation and Combination of Audio Tempo Estimation Approaches.pdf:pdf},
pages = {198 -- 207},
title = {{Comparative Evaluation and Combination of Audio Tempo Estimation Approaches}},
url = {http://www.aes.org/e-lib/browse.cfm?elib=15964},
year = {2011}
}
@inproceedings{Zapata2012,
abstract = {In this paper we establish a threshold for perceptually acceptable beat tracking based on the mutual agreement of a committee of beat trackers. In the ﬁrst step we use an existing annotated dataset to show that mutual agreement can be used to select one committee member as the most reliable beat tracker for a song. Then we conduct a listening test using a subset of the Million Song Dataset to establish a threshold which results in acceptable quality of the chosen beat output. For both datasets, we obtain a percentage of trackable music of about 73{\%}, and we investigate which data tags are related to acceptable and problematic beat tracking. The results indicate that current datasets are biased towards genres which tend to be easy for beat tracking. The proposed methods provide a means to automatically obtain a conﬁdence value for beat tracking in nonannotated data and to choose between a number of beat tracker outputs.},
author = {Zapata, Jose R and Holzapfel, Andr{\'{e}} and Davies, Matthew EP and Oliveira, Jo{\~{a}}o L and Gouyon, Fabien},
booktitle = {Proc. of the Int. Conf. on Music Information Retrieval (ISMIR)},
file = {:Users/ffont/Documents/Mendeley Desktop/Zapata, Holzapfel - 2012 - Assigning a Confidence Threshold on Automatic Beat Annotation in Large Datasets.pdf:pdf},
pages = {157--162},
title = {{Assigning a Confidence Threshold on Automatic Beat Annotation in Large Datasets}},
url = {http://telecom.inescporto.pt/{~}mdavies/pdfs/ZapataEtAl12-ismir.pdf},
year = {2012}
}
